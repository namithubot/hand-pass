{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRXrUrT7vpJm"
      },
      "source": [
        "##### Copyright 2023 The MediaPipe Authors. All Rights Reserved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32O3JsNL7AxT"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4up5flpBKPnh"
      },
      "source": [
        "Prerequisite: Upload the gesture data folder to the files here. It can be found in the code base"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -r hand_pass_model2"
      ],
      "metadata": {
        "id": "szFFJySwMXrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq gesture_data.zip"
      ],
      "metadata": {
        "id": "8FZ0iHkpmBNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKCar6AhDuwO",
        "outputId": "b5b75e83-8e77-4c6e-a532-9e4166e5f633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset/13\n",
            "['963.jpg', '1168.jpg', '939.jpg', '1045.jpg', '1028.jpg']\n",
            "dataset/15\n",
            "['963.jpg', '1168.jpg', '939.jpg', '1045.jpg', '1028.jpg']\n",
            "dataset/16\n",
            "['963.jpg', '1168.jpg', '939.jpg', '1045.jpg', '1028.jpg']\n",
            "dataset/14\n",
            "['963.jpg', '1168.jpg', '939.jpg', '1045.jpg', '1028.jpg']\n",
            "dataset/11\n",
            "['963.jpg', '1168.jpg', '939.jpg', '1045.jpg', '1028.jpg']\n",
            "dataset/12\n",
            "['963.jpg', '1168.jpg', '939.jpg', '1045.jpg', '1028.jpg']\n",
            "dataset/None\n",
            "['963.jpg', '1168.jpg', '939.jpg', '1045.jpg', '1028.jpg']\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "NUM_EXAMPLES = 5\n",
        "IMAGES_PATH = \"dataset\" # \"rps_data_sample\"\n",
        "\n",
        "# Get the list of labels from the list of folder names.\n",
        "labels = []\n",
        "for i in os.listdir(IMAGES_PATH):\n",
        "  if os.path.isdir(os.path.join(IMAGES_PATH, i)):\n",
        "    labels.append(i)\n",
        "\n",
        "# Show the images.\n",
        "for label in labels:\n",
        "  label_dir = os.path.join(IMAGES_PATH, label)\n",
        "  print(label_dir)\n",
        "  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
        "  print(example_filenames)\n",
        "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n",
        "  for i in range(NUM_EXAMPLES):\n",
        "    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n",
        "    axs[i].get_xaxis().set_visible(False)\n",
        "    axs[i].get_yaxis().set_visible(False)\n",
        "  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmgcSMPeyk0P"
      },
      "source": [
        "## Making a New Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxKdCdj2y_IM",
        "outputId": "787e5eb3-6468-480f-b135-79c1e9866e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q mediapipe-model-maker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTCjgBETDqU0",
        "outputId": "4d3b676a-f82b-40aa-9e46-378c85c6987e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary modules.\n",
        "from mediapipe_model_maker import gesture_recognizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gesture_recognizer.ModelOptions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeixZFJnxmOE",
        "outputId": "613eb3ad-ba84-4f37-aa76-ceb0a1b9dc8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "mediapipe_model_maker.python.vision.gesture_recognizer.model_options.GestureRecognizerModelOptions"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "Xh33Bg6QC5cz",
        "outputId": "dcf84e03-3773-4d73-9d9b-90facf41b597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://storage.googleapis.com/mediapipe-assets/palm_detection_full.tflite to /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n",
            "Downloading https://storage.googleapis.com/mediapipe-assets/hand_landmark_full.tflite to /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No valid hand is detected.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-3c36b39bece8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the gesture image archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m data = gesture_recognizer.Dataset.from_folder(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdirname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGES_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgesture_recognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHandDataPreprocessingParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mediapipe_model_maker/python/vision/gesture_recognizer/dataset.py\u001b[0m in \u001b[0;36mfrom_folder\u001b[0;34m(cls, dirname, hparams)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mvalid_hand_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhand_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid_hand_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No valid hand is detected.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mvalid_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_gesture_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No valid hand is detected."
          ]
        }
      ],
      "source": [
        "# Load the gesture image archive.\n",
        "data = gesture_recognizer.Dataset.from_folder(\n",
        "    dirname=IMAGES_PATH,\n",
        "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
        ")\n",
        "\n",
        "# Split the archive into training, validation and test dataset.\n",
        "train_data, rest_data = data.split(0.80)\n",
        "validation_data, test_data = rest_data.split(0.50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE-CyQglHQD1",
        "outputId": "d49d4a1d-3b7b-4967-d423-64e90466cd95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_57\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " hand_embedding (InputLayer  [(None, 128)]             0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization_119 (B  (None, 128)               512       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " re_lu_119 (ReLU)            (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_119 (Dropout)       (None, 128)               0         \n",
            "                                                                 \n",
            " custom_gesture_recognizer_  (None, 7)                 903       \n",
            " out (Dense)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1415 (5.53 KB)\n",
            "Trainable params: 1159 (4.53 KB)\n",
            "Non-trainable params: 256 (1.00 KB)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Resuming from hand_pass_model/epoch_models/model-0010\n",
            "Epoch 1/10\n",
            "238/238 [==============================] - 2s 5ms/step - loss: 0.4360 - categorical_accuracy: 0.7479 - val_loss: 0.3408 - val_categorical_accuracy: 0.7000 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "238/238 [==============================] - 1s 4ms/step - loss: 0.4064 - categorical_accuracy: 0.7668 - val_loss: 0.3296 - val_categorical_accuracy: 0.7000 - lr: 9.9000e-04\n",
            "Epoch 3/10\n",
            "238/238 [==============================] - 1s 4ms/step - loss: 0.4103 - categorical_accuracy: 0.7626 - val_loss: 0.3464 - val_categorical_accuracy: 0.6833 - lr: 9.8010e-04\n",
            "Epoch 4/10\n",
            "238/238 [==============================] - 1s 4ms/step - loss: 0.4099 - categorical_accuracy: 0.7731 - val_loss: 0.3257 - val_categorical_accuracy: 0.7000 - lr: 9.7030e-04\n",
            "Epoch 5/10\n",
            "238/238 [==============================] - 1s 4ms/step - loss: 0.3983 - categorical_accuracy: 0.7689 - val_loss: 0.3369 - val_categorical_accuracy: 0.7000 - lr: 9.6060e-04\n",
            "Epoch 6/10\n",
            "238/238 [==============================] - 1s 4ms/step - loss: 0.3949 - categorical_accuracy: 0.7626 - val_loss: 0.3457 - val_categorical_accuracy: 0.6833 - lr: 9.5099e-04\n",
            "Epoch 7/10\n",
            "238/238 [==============================] - 1s 4ms/step - loss: 0.4079 - categorical_accuracy: 0.7773 - val_loss: 0.3640 - val_categorical_accuracy: 0.6833 - lr: 9.4148e-04\n",
            "Epoch 8/10\n",
            "238/238 [==============================] - 1s 4ms/step - loss: 0.4153 - categorical_accuracy: 0.7584 - val_loss: 0.3621 - val_categorical_accuracy: 0.6833 - lr: 9.3207e-04\n",
            "Epoch 9/10\n",
            "238/238 [==============================] - 1s 5ms/step - loss: 0.3915 - categorical_accuracy: 0.7710 - val_loss: 0.3487 - val_categorical_accuracy: 0.6833 - lr: 9.2274e-04\n",
            "Epoch 10/10\n",
            "238/238 [==============================] - 1s 4ms/step - loss: 0.3795 - categorical_accuracy: 0.7815 - val_loss: 0.3475 - val_categorical_accuracy: 0.6833 - lr: 9.1352e-04\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "hparams = gesture_recognizer.HParams(export_dir=\"hand_pass_model\")\n",
        "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the custom model\n",
        "hparams2 = gesture_recognizer.HParams(export_dir=\"hand_pass_model2\", epochs=10, learning_rate = 0.001, steps_per_epoch=128)\n",
        "custom_option = gesture_recognizer.ModelOptions(dropout_rate=0.3, layer_widths=[32])\n",
        "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams2, model_options=custom_option)\n",
        "model2 = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvTrs6sIziof",
        "outputId": "70402752-272b-4df5-804a-484f55848188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_79\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " hand_embedding (InputLayer  [(None, 128)]             0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization_163 (B  (None, 128)               512       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " re_lu_163 (ReLU)            (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_163 (Dropout)       (None, 128)               0         \n",
            "                                                                 \n",
            " custom_gesture_recognizer_  (None, 32)                4128      \n",
            " 0 (Dense)                                                       \n",
            "                                                                 \n",
            " batch_normalization_164 (B  (None, 32)                128       \n",
            " atchNormalization)                                              \n",
            "                                                                 \n",
            " re_lu_164 (ReLU)            (None, 32)                0         \n",
            "                                                                 \n",
            " dropout_164 (Dropout)       (None, 32)                0         \n",
            "                                                                 \n",
            " custom_gesture_recognizer_  (None, 7)                 231       \n",
            " out (Dense)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4999 (19.53 KB)\n",
            "Trainable params: 4679 (18.28 KB)\n",
            "Non-trainable params: 320 (1.25 KB)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Resuming from hand_pass_model2/epoch_models/model-0100\n",
            "Epoch 1/10\n",
            "128/128 [==============================] - 2s 8ms/step - loss: 0.4247 - categorical_accuracy: 0.7148 - val_loss: 0.3259 - val_categorical_accuracy: 0.7167 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4341 - categorical_accuracy: 0.7305 - val_loss: 0.3381 - val_categorical_accuracy: 0.7167 - lr: 9.9000e-04\n",
            "Epoch 3/10\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4748 - categorical_accuracy: 0.7344 - val_loss: 0.3677 - val_categorical_accuracy: 0.7167 - lr: 9.8010e-04\n",
            "Epoch 4/10\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.4365 - categorical_accuracy: 0.7656 - val_loss: 0.3697 - val_categorical_accuracy: 0.7167 - lr: 9.7030e-04\n",
            "Epoch 5/10\n",
            "128/128 [==============================] - 1s 8ms/step - loss: 0.4448 - categorical_accuracy: 0.7500 - val_loss: 0.3826 - val_categorical_accuracy: 0.7000 - lr: 9.6060e-04\n",
            "Epoch 6/10\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.3977 - categorical_accuracy: 0.7617 - val_loss: 0.3792 - val_categorical_accuracy: 0.7167 - lr: 9.5099e-04\n",
            "Epoch 7/10\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4832 - categorical_accuracy: 0.6992 - val_loss: 0.3449 - val_categorical_accuracy: 0.7000 - lr: 9.4148e-04\n",
            "Epoch 8/10\n",
            "128/128 [==============================] - 1s 7ms/step - loss: 0.4609 - categorical_accuracy: 0.6992 - val_loss: 0.3764 - val_categorical_accuracy: 0.7167 - lr: 9.3207e-04\n",
            "Epoch 9/10\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4693 - categorical_accuracy: 0.7461 - val_loss: 0.3589 - val_categorical_accuracy: 0.7167 - lr: 9.2274e-04\n",
            "Epoch 10/10\n",
            "128/128 [==============================] - 1s 6ms/step - loss: 0.4433 - categorical_accuracy: 0.7383 - val_loss: 0.3458 - val_categorical_accuracy: 0.7167 - lr: 9.1352e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the details of the models"
      ],
      "metadata": {
        "id": "1XaypNAn0XwQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULANbzL7HnIR",
        "outputId": "7b3a6b0c-0244-4c22-eb85-80b76dddcd34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60/60 [==============================] - 0s 2ms/step - loss: 0.1754 - categorical_accuracy: 0.8833\n",
            "Test loss:0.17535260319709778, Test accuracy:0.8833333253860474\n"
          ]
        }
      ],
      "source": [
        "loss, acc = model2.evaluate(test_data, batch_size=1)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_data, batch_size=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7QsQHIZ9mG4",
        "outputId": "3af667e5-f517-4807-99c1-2e535c50822b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60/60 [==============================] - 1s 2ms/step - loss: 0.1736 - categorical_accuracy: 0.8500\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1736307293176651, 0.8500000238418579]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.evaluate(test_data, batch_size=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeE9YSxS4h00",
        "outputId": "2ab56317-5690-4b25-ecd3-7ff18805906e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60/60 [==============================] - 1s 2ms/step - loss: 0.3998 - categorical_accuracy: 0.8667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.39977094531059265, 0.8666666746139526]"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4q8Ukqu0IC0",
        "outputId": "eb9baea6-acfd-4c2d-80cf-0f4a6e9d70cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " hand_embedding (InputLayer  [(None, 128)]             0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 128)               512       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " re_lu (ReLU)                (None, 128)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " custom_gesture_recognizer_  (None, 7)                 903       \n",
            " out (Dense)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1415 (5.53 KB)\n",
            "Trainable params: 1159 (4.53 KB)\n",
            "Non-trainable params: 256 (1.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMZQc767I9Q-",
        "outputId": "fd30c857-fed4-4369-d2f7-789f7320e7b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://storage.googleapis.com/mediapipe-assets/gesture_embedder.tflite to /tmp/model_maker/gesture_recognizer/gesture_embedder.tflite\n",
            "Using existing files at /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n",
            "Using existing files at /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n",
            "Downloading https://storage.googleapis.com/mediapipe-assets/canned_gesture_classifier.tflite to /tmp/model_maker/gesture_recognizer/canned_gesture_classifier.tflite\n"
          ]
        }
      ],
      "source": [
        "# Export the model bundle.\n",
        "model.export_model()\n",
        "# Rename the file to be more descriptive.\n",
        "!mv hand_pass_model/gesture_recognizer.task hand_pass.task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pHxU6SjoLW-s",
        "outputId": "00085767-26f9-4f60-cad1-859b037a2ef9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_01bc0b87-64e7-49e4-b6ee-541ffedb5095\", \"hand_pass.task\", 8462425)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download(\"hand_pass.task\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}